{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DF=pd.read_csv('nlpdata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Content    8\n",
       "Topic      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete nan values\n",
    "DF.dropna(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Content    0\n",
       "Topic      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "#sns.countplot(x= 'Topic',data = DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize \n",
    "\n",
    "def normalizer(sen):\n",
    "    sen = re.sub(r'[^\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD ]+', '', sen)# keep only arabic letters and spaces \n",
    "    sen=re.sub(r'\\b\\w{1,3}\\b', ' ', sen).strip() #delete words that less than 4 letter\n",
    "    tokens=regexp_tokenize(sen, \"[\\w']+\") #tekonize words \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['يتفرع',\n",
       " 'الإنسان',\n",
       " 'الإنسان',\n",
       " 'الاجتماعي',\n",
       " 'الذي',\n",
       " 'يدرس',\n",
       " 'تصرفات',\n",
       " 'البشر',\n",
       " 'المعاصرين',\n",
       " 'وعلم',\n",
       " 'الإنسان',\n",
       " 'الثقافي',\n",
       " 'الذي',\n",
       " 'يدرس',\n",
       " 'بناء',\n",
       " 'الثقافات',\n",
       " 'البشرية',\n",
       " 'وأداءها',\n",
       " 'وظائفها',\n",
       " 'زمان',\n",
       " 'ومكان',\n",
       " 'وعلم',\n",
       " 'الأنثروبولوجيا',\n",
       " 'اللغوية',\n",
       " 'الذي',\n",
       " 'يدرس',\n",
       " 'تأثير',\n",
       " 'اللغة',\n",
       " 'الحياة',\n",
       " 'الاجتماعية',\n",
       " 'وعلم',\n",
       " 'الإنسان',\n",
       " 'الحيوي',\n",
       " 'الذي',\n",
       " 'يدرس',\n",
       " 'تطور',\n",
       " 'الإنسان',\n",
       " 'بيولوجي',\n",
       " 'الآثار',\n",
       " 'الذي',\n",
       " 'يدرس',\n",
       " 'ثقافات',\n",
       " 'البشر',\n",
       " 'القديمة',\n",
       " 'بالتحقيق',\n",
       " 'الأدلة',\n",
       " 'المادية',\n",
       " 'فيعد',\n",
       " 'الإنسان',\n",
       " 'الولايات',\n",
       " 'المتحدة',\n",
       " 'بينما',\n",
       " 'إليه',\n",
       " 'أوروبا',\n",
       " 'منفصل',\n",
       " 'بذاته',\n",
       " 'أقرب',\n",
       " 'التاريخ',\n",
       " 'الأنثروبولوجيا']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example of normalizer application on our Data\n",
    "DF['normalized'] = DF.Content.apply(normalizer)\n",
    "DF['normalized'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# create a pipeline that will automate modeling workflow: apply normalizer function+create Bag of Words(BoW)+use algorithm of Naive Bayes:\n",
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer=normalizer)),  # normalized strings to token integer counts\n",
    "    ('classifier', MultinomialNB()) # train on vectors w/ Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(DF['Content'],DF['Topic'], test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function normalizer at 0x0000019F16FCC558>)),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "y_pred= pipeline.predict(X_test)\n",
    "#let's see our classification report\n",
    "#print(classification_report(y_test, y_pred, target_names=list(DF.Topic.unique())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see our confusion matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt     \n",
    "cm=confusion_matrix(y_test, y_pred, labels=['Anthropology','Astronomy'])\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); \n",
    "ax.xaxis.set_ticklabels(['Anthropology','Astronomy']); ax.yaxis.set_ticklabels(['Anthropology','Astronomy']);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's test our algorithm:\n",
    "mytext='إن تباين مفهوم (علم الكون) وفقًا لحجم كتابة أول حرف من تهجئته، يوحي بأن هناك أكثر من كون تم إيجاده في هذه الحياة، وهو أمر مستحيل بالطبيعة، فلا يوجد في هذه الحياة سوى كون واحد، وذلك يضعنا في مشكلة كيفية إمكان تطبيق تجارب مختلفة لأكثر من مفهوم على كون واحد'\n",
    "myprediction= pipeline.predict([mytext])\n",
    "print(myprediction[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(pipeline,open('model.pkl',\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
